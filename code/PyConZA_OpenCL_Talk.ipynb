{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel Programming with (Py)OpenCL for Fun and Profit\n",
    "\n",
    "Gordon Inggs\n",
    "\n",
    "Github: *Gordonei*\n",
    "\n",
    "Day Job(s): \n",
    "* Science Data, City of Cape Town\n",
    "* Application Acceleration Consultant, My Lounge/Back Garden*\n",
    "\n",
    "\\*weather permitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Talk Outline\n",
    "* Why PyOpenCL (~5 mins)\n",
    "* How to *x* with PyOpenCL (~15 mins)\n",
    "  * Program CPUs, GPUs and more\n",
    "  * Manipulate Memory\n",
    "  * Do things ~~concurrently~~ in parallel\n",
    "  \n",
    "Github Repo: [Gordonei/pyconza-october-2018](https://github.com/Gordonei/pyconza-october-2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Talk Approach\n",
    "Use trusty vector sum example to illustrate points. Points should be a tight loop between imparting some information, then a demonstration of that information in action.\n",
    "\n",
    "Encourage interruptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why PyOpenCL? ( $\\approx$ Why Heterogeneous Compute? )\n",
    " \n",
    "![Cat Lion Gif](./img/cat_lion.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bad News: The Free Lunch is over\n",
    "\n",
    "![Cat sad gif](./img/cat_sad.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../img/freelunch_is_over.png\" alt=\"Free Lunch is over graph\" width=\"500\"/>\n",
    "\n",
    "*Source: Herb Sutter, The Free Lunch is Over*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Good New: There are alternatives \n",
    "\n",
    "![Cat milk gif](./img/cat_milk.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Performance-wise\n",
    "\n",
    "![Double precision performance graph](./img/performance_graph.png)\n",
    "\n",
    "*Source: Karl Rupp, ViennaCL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Energy-wise\n",
    "\n",
    "![Power performance graph](./img/power_graph.png)\n",
    "\n",
    "*Source: Karl Rupp, ViennaCL*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Performance vs Energy\n",
    "\n",
    "![Bitcoin mining performance graph](./img/performance_graph_II.png)\n",
    "\n",
    "*Source: @oocBlog, The cost of securing the network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sounds great, but...\n",
    "\n",
    "![Cats on turntable](./img/cat_turntable.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# The Challenges of Heterogeneous Computing\n",
    "1. The Orientation Problem - turning things on!\n",
    "2. The IO Problem - moving data around!\n",
    "3. The Conceptual Problem - what am I doing?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "PyOpenCL helps us overcome these challenges:\n",
    "1. Program Multicore CPUs and GPUs. Also allows for device characteristics at runtime.\n",
    "2. Allows for the efficient transfer of data. Exposes device memory hierachy.\n",
    "3. Exposes task vs data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programming Fancy Devices\n",
    "\n",
    "In this section:\n",
    "* What an OpenCL program is\n",
    "* How to compile an OpenCL program\n",
    "* How to run an OpenCL program\n",
    "\n",
    "![Cat Metro Mayer gif](./img/cat_metro_mayer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What is OpenCL?\n",
    "\n",
    "‘OpenCL (Open Computing Language) is the open, royalty-free standard\n",
    "for cross-platform, parallel programming of diverse processors found in\n",
    "personal computers, servers, mobile devices and embedded platforms.’\n",
    "\n",
    "Intended for portable Heterogeneous computing, but not performance portability\n",
    "\n",
    "2 APIs (programming and runtime) and a kernel language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Khronos Group Promoters\n",
    "\n",
    "![Khronos Group Promoters](./img/khronos_group_promoters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting platforms &/ devices\n",
    "Looking at what is available, then selecting the NVIDIA one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyopencl\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel(R) OpenCL\n",
      "Portable Computing Language\n",
      "NVIDIA CUDA\n"
     ]
    }
   ],
   "source": [
    "ocl_platforms = (platform.name \n",
    "                 for platform in pyopencl.get_platforms())\n",
    "print(\"\\n\".join(ocl_platforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nvidia_platform = [platform \n",
    "                   for platform in pyopencl.get_platforms() \n",
    "                   if platform.name == \"NVIDIA CUDA\"][0]\n",
    "nvidia_devices = nvidia_platform.get_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## OpenCL Programming Abstractions\n",
    "\n",
    "![OpenCL Programming abstractions](./img/opencl_programming_abstractions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building Programs\n",
    "Using the PyOpenCL bindings to create an OpenCL context, then declaring OpenCL kernel code, and compiling it.\n",
    "\n",
    "The code is for a simple vector sum, i.e. $\\vec{c} = \\vec{a} + \\vec{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nvidia_context = pyopencl.Context(devices=nvidia_devices)\n",
    "\n",
    "program_source = \"\"\"\n",
    "      kernel void sum(global float *a, \n",
    "                      global float *b,\n",
    "                      global float *c){\n",
    "        int gid = get_global_id(0);\n",
    "        c[gid] = a[gid] + b[gid];\n",
    "      }\n",
    "    \"\"\"\n",
    "nvidia_program_source = pyopencl.Program(nvidia_context, program_source)\n",
    "nvidia_program = nvidia_program_source.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Names: sum\n"
     ]
    }
   ],
   "source": [
    "program_kernel_names = nvidia_program.get_info(pyopencl.program_info.KERNEL_NAMES)\n",
    "print(\"Kernel Names: {}\".format(program_kernel_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## OpenCL Runtime Abstractions\n",
    "\n",
    "![OpenCL Runtime abstractions](./img/opencl_runtime_abstractions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running Programs\n",
    "Actually running the code using the Python bindings. \n",
    "\n",
    "Some memory and data needs to be setup first, but I'll elaborate on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def run_ocl_kernel(queue, kernel, global_size,\n",
    "                   input_tuples, output_tuples,\n",
    "                   local_size = (32,)):\n",
    "    \n",
    "    # copying data onto the device\n",
    "    for (array, buffer) in input_tuples:\n",
    "        pyopencl.enqueue_copy(queue, src=array, dest=buffer)\n",
    "    \n",
    "    # running program on the device\n",
    "    kernel_arguments  = [buffer for (_,buffer) in input_tuples] \n",
    "    kernel_arguments += [buffer for (_,buffer) in output_tuples]\n",
    "        \n",
    "    kernel(queue, global_size, local_size,\n",
    "           *kernel_arguments)\n",
    "\n",
    "    # copying data off the device\n",
    "    for (arr, buffer) in output_tuples:\n",
    "        pyopencl.enqueue_copy(queue, src=buffer, dest=arr)\n",
    "        \n",
    "    # waiting for everything to finish\n",
    "    queue.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trust, but verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def check_sum_results(a,b,c):\n",
    "    c_ref = a + b\n",
    "    err = numpy.abs(c - c_ref)\n",
    "    if((err.sum() > 0.0).any()): \n",
    "        print(\"result does not match\")\n",
    "    else: \n",
    "        print(\"result matches!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic data setup\n",
    "N = int(2**20)\n",
    "a = numpy.random.rand(N).astype(numpy.float32)\n",
    "b = numpy.random.rand(N).astype(numpy.float32)\n",
    "c = numpy.empty_like(a)\n",
    "\n",
    "# Device Memory setup\n",
    "a_nvidia_buffer = pyopencl.Buffer(nvidia_context,\n",
    "                                  flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                  size=a.nbytes)\n",
    "b_nvidia_buffer = pyopencl.Buffer(nvidia_context, \n",
    "                                  flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                  size=b.nbytes)\n",
    "c_nvidia_buffer = pyopencl.Buffer(nvidia_context, \n",
    "                                  flags=pyopencl.mem_flags.WRITE_ONLY, \n",
    "                                  size=c.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nvidia_queue = pyopencl.CommandQueue(nvidia_context)\n",
    "\n",
    "input_tuples = ((a, a_nvidia_buffer), (b, b_nvidia_buffer), )\n",
    "output_tuples = ((c, c_nvidia_buffer),)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "check_sum_results(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.65 ms ± 507 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "***Suggestion***: Characterise the OpenCL devices you can get access to, stepping up the size of $N$ by some factor (power of 2 is easy to reason about). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Manipulate Memory\n",
    "\n",
    "In this section:\n",
    "* Moving data between the host and OpenCL device\n",
    "* Specifying storage on the device being used\n",
    "\n",
    "![Cat box gif](./img/cat_box.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## OpenCL Memory Abstractions\n",
    "\n",
    "![OpenCL Memory](./img/opencl_memory_abstractions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using global memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_input_memory(context, input_arrays):\n",
    "    return [(array, pyopencl.Buffer(context,\n",
    "                                flags=pyopencl.mem_flags.READ_ONLY, \n",
    "                                size=array.nbytes))\n",
    "        for array in input_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_output_memory(context, output_arrays):\n",
    "    return [(array, pyopencl.Buffer(context,\n",
    "                                flags=pyopencl.mem_flags.WRITE_ONLY, \n",
    "                                size=array.nbytes))\n",
    "        for array in output_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_memories(tuples):\n",
    "    for (_, buffer) in tuples:\n",
    "        buffer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "a = numpy.random.rand(N).astype(numpy.float32)\n",
    "b = numpy.random.rand(N).astype(numpy.float32)\n",
    "c = numpy.empty_like(a)\n",
    "\n",
    "# Device Memory setup\n",
    "input_tuples = create_input_memory(nvidia_context, (a,b,))\n",
    "output_tuples = create_output_memory(nvidia_context, (c,),)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples)\n",
    "check_sum_results(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def build_program(ocl_file, context, build_options=[]):\n",
    "    with open(ocl_file,\"r\") as opencl_file:\n",
    "        program_source_code = opencl_file.read()\n",
    "        program_source = pyopencl.Program(context, program_source_code)\n",
    "        program = program_source.build(options=build_options)\n",
    "        \n",
    "        return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result matches!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "wg_size = 32\n",
    "power = 1\n",
    "build_options = [\"-D\",\"BATCH_SIZE={}\".format(batch_size),\n",
    "                 \"-D\",\"WG_SIZE={}\".format(wg_size),\n",
    "                 \"-D\",\"POW={}\".format(power)]\n",
    "\n",
    "nvidia_program = build_program(\"vector_sum.cl\", nvidia_context, build_options)\n",
    "run_ocl_kernel(nvidia_queue, nvidia_program.sum, (N,), input_tuples, output_tuples, (wg_size,))\n",
    "check_sum_results(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batching\n",
    "\n",
    "```c\n",
    "kernel void sum_batched(global float *a, \n",
    "                        global float *b,\n",
    "                        global float *c){\n",
    "  int gid = get_global_id(0)*BATCH_SIZE;\n",
    "    \n",
    "  for(int i=0; i<BATCH_SIZE;++i)\n",
    "     c[gid + i] = a[gid + i] + b[gid + i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.62 ms ± 543 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum_batched, (N//batch_size,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using local and private memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batched + Private Memory\n",
    "\n",
    "```c\n",
    "kernel void sum_batched_private(global float *a, \n",
    "                                global float *b,\n",
    "                                global float *c){\n",
    "  int gid = get_global_id(0)*BATCH_SIZE;\n",
    "    \n",
    "  float a_tmp[BATCH_SIZE], b_tmp[BATCH_SIZE], c_tmp[BATCH_SIZE];\n",
    "      \n",
    "  for(int i=0; i<BATCH_SIZE;++i){\n",
    "    a_tmp[i] = a[gid + i];\n",
    "    b_tmp[i] = b[gid + i];\n",
    "  }\n",
    "    \n",
    "  for(int i=0; i<BATCH_SIZE;++i)\n",
    "    c_tmp[i] = a_tmp[i] + b_tmp[i];\n",
    "    \n",
    "  for(int i=0; i<BATCH_SIZE;++i) \n",
    "    c[gid + i] = c_tmp[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.53 ms ± 549 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum_batched_private, (N//batch_size,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Do Things in Parallel\n",
    "\n",
    "In this section:\n",
    "* Inspecting the characteristics of the available devices\n",
    "* Use the work-items and work-groups abstraction to chose between task and data parallelism.\n",
    "\n",
    "![Cat yarn gif](./img/cat_yarn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Device characteristics\n",
    "Using the host code API to inspect the characteristics of the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "name_properties = {\n",
    "    \"Device Name\": pyopencl.device_info.NAME,\n",
    "    \"Device Platform\": pyopencl.device_info.PLATFORM,\n",
    "    \"Device Type\": pyopencl.device_info.TYPE\n",
    "}\n",
    "\n",
    "processing_properties = {\n",
    "    \"Available Compute Units\": pyopencl.device_info.MAX_COMPUTE_UNITS,\n",
    "    \"Clockrate\": pyopencl.device_info.MAX_CLOCK_FREQUENCY\n",
    "}\n",
    "\n",
    "memory_properties = {\n",
    "    \"Available Global Memory\": pyopencl.device_info.GLOBAL_MEM_SIZE,\n",
    "    \"Available Constant Memory\": pyopencl.device_info.MAX_CONSTANT_BUFFER_SIZE,\n",
    "    \"Available Local Memory\" : pyopencl.device_info.LOCAL_MEM_SIZE\n",
    "}\n",
    "\n",
    "device_types = {\n",
    "    pyopencl.device_type.CPU: \"CPU\",\n",
    "    pyopencl.device_type.GPU: \"GPU\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "intel_platform = [platform for platform in pyopencl.get_platforms() \n",
    "                  if platform.name == \"Intel(R) OpenCL\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: Intel(R) HD Graphics\n",
      "Device Platform: <pyopencl.Platform 'Intel(R) OpenCL' at 0x321d400>\n",
      "Device Types: GPU\n",
      "Available Compute Units: 23\n",
      "Clockrate: 1100\n",
      "Available Constant Memory: 4294959103\n",
      "Available Global Memory: 13321633792\n",
      "Available Local Memory: 65536\n",
      "\n",
      "\n",
      "Device Name: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "Device Platform: <pyopencl.Platform 'Intel(R) OpenCL' at 0x321d400>\n",
      "Device Types: CPU\n",
      "Available Compute Units: 8\n",
      "Clockrate: 2800\n",
      "Available Constant Memory: 131072\n",
      "Available Global Memory: 16662528000\n",
      "Available Local Memory: 32768\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for device in intel_platform.get_devices():\n",
    "    #print out all of the device name properties, except the device type\n",
    "    for property_name in sorted(name_properties.keys() - {\"Device Type\"}):\n",
    "        property_string_args = (property_name,device.get_info(name_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "        \n",
    "    #look up the device type\n",
    "    print(\"Device Types: {}\".format(device_types[device.get_info(name_properties[\"Device Type\"])]))\n",
    "    \n",
    "    #print out all of the processing properties\n",
    "    for property_name in sorted(processing_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(processing_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "    \n",
    "    #print out all of the memory properties\n",
    "    for property_name in sorted(memory_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(memory_properties[property_name]))\n",
    "        print(\"{}: {}\".format(*property_string_args))\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               3\n",
      "  Platform Name                                   Intel(R) OpenCL\n",
      "  Platform Vendor                                 Intel(R) Corporation\n",
      "  Platform Version                                OpenCL 2.0 \n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_icd cl_khr_image2d_from_buffer cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_spir\n",
      "  Platform Extensions function suffix             INTEL\n",
      "\n",
      "  Platform Name                                   Portable Computing Language\n",
      "  Platform Vendor                                 The pocl project\n",
      "  Platform Version                                OpenCL 1.2 pocl 1.1 None+Asserts, LLVM 6.0.0, SPIR, SLEEF, DISTRO, POCL_DEBUG\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_icd\n",
      "  Platform Extensions function suffix             POCL\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 1.2 CUDA 9.1.84\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_nv_create_buffer\n",
      "  Platform Extensions function suffix             NV\n",
      "\n",
      "  Platform Name                                   Intel(R) OpenCL\n",
      "Number of devices                                 2\n",
      "  Device Name                                     Intel(R) HD Graphics\n",
      "  Device Vendor                                   Intel(R) Corporation\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 2.0 \n",
      "  Driver Version                                  r5.0.63503\n",
      "  Device OpenCL C Version                         OpenCL C 2.0 \n",
      "  Device Type                                     GPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               23\n",
      "  Max clock frequency                             1100MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     0\n",
      "    Supported partition types                     by <unknown> (0x558D00000000)\n",
      "    Supported affinity domains                    0x558D00000000\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             256x256x256\n",
      "  Max work group size                             256\n",
      "  Preferred work group size multiple              32\n",
      "  Sub-group sizes (Intel)                         8, 16, 32\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                16 / 16      \n",
      "    short                                                8 / 8       \n",
      "    int                                                  4 / 4       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 8 / 8        (cl_khr_fp16)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (cl_khr_fp16)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              13321633792 (12.41GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4294959103 (4GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           64 bytes\n",
      "    Global                                        64 bytes\n",
      "    Local                                         64 bytes\n",
      "  Max size for global variable                    65536 (64KiB)\n",
      "  Preferred total size of global vars             4294959103 (4GiB)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        524288 (512KiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             16\n",
      "    Max size for 1D images from buffer            268434943 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   4 bytes\n",
      "    Pitch alignment for 2D image buffers          4 pixels\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max planar YUV image size                     16384x16380 pixels\n",
      "    Max 3D image size                             16384x16384x2048 pixels\n",
      "    Max number of read image args                 128\n",
      "    Max number of write image args                128\n",
      "    Max number of read/write image args           128\n",
      "  Max number of pipe args                         16\n",
      "  Max active pipe reservations                    1\n",
      "  Max pipe packet size                            1024\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               65536 (64KiB)\n",
      "  Max number of constant args                     8\n",
      "  Max constant buffer size                        4294959103 (4GiB)\n",
      "  Max size of kernel argument                     1024\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Preferred size                                131072 (128KiB)\n",
      "    Max size                                      67108864 (64MiB)\n",
      "  Max queues on device                            1\n",
      "  Max events on device                            1024\n",
      "  Prefer user sync for interop                    Yes\n",
      "  Profiling timer resolution                      83ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    SPIR versions                                 1.2 \n",
      "  printf() buffer size                            4194304 (4MiB)\n",
      "  Built-in kernels                                block_motion_estimate_intel;block_advanced_motion_estimate_check_intel;block_advanced_motion_estimate_bidirectional_check_intel\n",
      "  Motion Estimation accelerator version (Intel)   2\n",
      "    Device-side AVC Motion Estimation version     1\n",
      "      Supports texture sampler use                Yes\n",
      "      Supports preemption                         No\n",
      "  Device Extensions                               cl_intel_accelerator cl_intel_advanced_motion_estimation cl_intel_device_side_avc_motion_estimation cl_intel_driver_diagnostics cl_intel_media_block_io cl_intel_motion_estimation cl_intel_planar_yuv cl_intel_packed_yuv cl_intel_required_subgroup_size cl_intel_subgroups cl_intel_subgroups_short cl_intel_va_api_media_sharing cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_fp16 cl_khr_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_icd cl_khr_image2d_from_buffer cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_mipmap_image cl_khr_mipmap_image_writes cl_khr_spir cl_khr_subgroups \n",
      "\n",
      "  Device Name                                     Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "  Device Vendor                                   Intel(R) Corporation\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 2.0 (Build 475)\n",
      "  Driver Version                                  1.2.0.475\n",
      "  Device OpenCL C Version                         OpenCL C 2.0 \n",
      "  Device Type                                     CPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               8\n",
      "  Max clock frequency                             2800MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     8\n",
      "    Supported partition types                     by counts, equally, by names (Intel)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             8192x8192x8192\n",
      "  Max work group size                             8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preferred work group size multiple              128\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 32      \n",
      "    short                                                1 / 16      \n",
      "    int                                                  1 / 8       \n",
      "    long                                                 1 / 4       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 8       \n",
      "    double                                               1 / 4        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 No\n",
      "    Round to infinity                             No\n",
      "    IEEE754-2008 fused multiply-add               No\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  No\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              16662528000 (15.52GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4165632000 (3.88GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           64 bytes\n",
      "    Global                                        64 bytes\n",
      "    Local                                         0 bytes\n",
      "  Max size for global variable                    65536 (64KiB)\n",
      "  Preferred total size of global vars             65536 (64KiB)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        262144 (256KiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             480\n",
      "    Max size for 1D images from buffer            260352000 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   64 bytes\n",
      "    Pitch alignment for 2D image buffers          64 pixels\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max 3D image size                             2048x2048x2048 pixels\n",
      "    Max number of read image args                 480\n",
      "    Max number of write image args                480\n",
      "    Max number of read/write image args           480\n",
      "  Max number of pipe args                         16\n",
      "  Max active pipe reservations                    32767\n",
      "  Max pipe packet size                            1024\n",
      "  Local memory type                               Global\n",
      "  Local memory size                               32768 (32KiB)\n",
      "  Max number of constant args                     480\n",
      "  Max constant buffer size                        131072 (128KiB)\n",
      "  Max size of kernel argument                     3840 (3.75KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Local thread execution (Intel)                Yes\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "    Preferred size                                4294967295 (4GiB)\n",
      "    Max size                                      4294967295 (4GiB)\n",
      "  Max queues on device                            4294967295\n",
      "  Max events on device                            4294967295\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            Yes\n",
      "    SPIR versions                                 1.2\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                \n",
      "  Device Extensions                               cl_khr_icd cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_depth_images cl_khr_3d_image_writes cl_intel_exec_by_local_thread cl_khr_spir cl_khr_fp64 cl_khr_image2d_from_buffer \n",
      "\n",
      "  Platform Name                                   Portable Computing Language\n",
      "Number of devices                                 1\n",
      "  Device Name                                     pthread-Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz\n",
      "  Device Vendor                                   GenuineIntel\n",
      "  Device Vendor ID                                0x8086\n",
      "  Device Version                                  OpenCL 1.2 pocl HSTR: pthread-x86_64-pc-linux-gnu-skylake\n",
      "  Driver Version                                  1.1\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 pocl\n",
      "  Device Type                                     CPU\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               8\n",
      "  Max clock frequency                             3800MHz\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     8\n",
      "    Supported partition types                     equally, by counts\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             4096x4096x4096\n",
      "  Max work group size                             4096\n",
      "  Preferred work group size multiple              8\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                16 / 16      \n",
      "    short                                               16 / 16      \n",
      "    int                                                  8 / 8       \n",
      "    long                                                 4 / 4       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                8 / 8       \n",
      "    double                                               4 / 4        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  Global memory size                              14515044352 (13.52GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           4294967296 (4GiB)\n",
      "  Unified memory for Host and Device              Yes\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       1024 bits (128 bytes)\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        6291456 (6MiB)\n",
      "  Global Memory cache line size                   64 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             16\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Max 2D image size                             16384x16384 pixels\n",
      "    Max 3D image size                             2048x2048x2048 pixels\n",
      "    Max number of read image args                 128\n",
      "    Max number of write image args                128\n",
      "  Local memory type                               Global\n",
      "  Local memory size                               4194304 (4MiB)\n",
      "  Max number of constant args                     8\n",
      "  Max constant buffer size                        4194304 (4MiB)\n",
      "  Max size of kernel argument                     1024\n",
      "  Queue properties                                \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     Yes\n",
      "  Prefer user sync for interop                    Yes\n",
      "  Profiling timer resolution                      1ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            Yes\n",
      "    SPIR versions                                 1.2\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                \n",
      "  Device Extensions                               cl_khr_byte_addressable_store cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_3d_image_writes cl_khr_spir cl_khr_fp64 cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp64\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     GeForce GTX 1050\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 1.2 CUDA\n",
      "  Driver Version                                  390.48\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               5\n",
      "  Max clock frequency                             1493MHz\n",
      "  Compute Capability (NV)                         6.1\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preferred work group size multiple              32\r\n",
      "  Warp size (NV)                                  32\r\n",
      "  Preferred / native vector sizes                 \r\n",
      "    char                                                 1 / 1       \r\n",
      "    short                                                1 / 1       \r\n",
      "    int                                                  1 / 1       \r\n",
      "    long                                                 1 / 1       \r\n",
      "    half                                                 0 / 0        (n/a)\r\n",
      "    float                                                1 / 1       \r\n",
      "    double                                               1 / 1        (cl_khr_fp64)\r\n",
      "  Half-precision Floating-point support           (n/a)\r\n",
      "  Single-precision Floating-point support         (core)\r\n",
      "    Denormals                                     Yes\r\n",
      "    Infinity and NANs                             Yes\r\n",
      "    Round to nearest                              Yes\r\n",
      "    Round to zero                                 Yes\r\n",
      "    Round to infinity                             Yes\r\n",
      "    IEEE754-2008 fused multiply-add               Yes\r\n",
      "    Support is emulated in software               No\r\n",
      "    Correctly-rounded divide and sqrt operations  Yes\r\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\r\n",
      "    Denormals                                     Yes\r\n",
      "    Infinity and NANs                             Yes\r\n",
      "    Round to nearest                              Yes\r\n",
      "    Round to zero                                 Yes\r\n",
      "    Round to infinity                             Yes\r\n",
      "    IEEE754-2008 fused multiply-add               Yes\r\n",
      "    Support is emulated in software               No\r\n",
      "  Address bits                                    64, Little-Endian\r\n",
      "  Global memory size                              4238737408 (3.948GiB)\r\n",
      "  Error Correction support                        No\r\n",
      "  Max memory allocation                           1059684352 (1011MiB)\r\n",
      "  Unified memory for Host and Device              No\r\n",
      "  Integrated memory (NV)                          No\r\n",
      "  Minimum alignment for any data type             128 bytes\r\n",
      "  Alignment of base address                       4096 bits (512 bytes)\r\n",
      "  Global Memory cache type                        Read/Write\r\n",
      "  Global Memory cache size                        81920 (80KiB)\r\n",
      "  Global Memory cache line size                   128 bytes\r\n",
      "  Image support                                   Yes\r\n",
      "    Max number of samplers per kernel             32\r\n",
      "    Max size for 1D images from buffer            134217728 pixels\r\n",
      "    Max 1D or 2D image array size                 2048 images\r\n",
      "    Max 2D image size                             16384x32768 pixels\r\n",
      "    Max 3D image size                             16384x16384x16384 pixels\r\n",
      "    Max number of read image args                 256\r\n",
      "    Max number of write image args                16\r\n",
      "  Local memory type                               Local\r\n",
      "  Local memory size                               49152 (48KiB)\r\n",
      "  Registers per block (NV)                        65536\r\n",
      "  Max number of constant args                     9\r\n",
      "  Max constant buffer size                        65536 (64KiB)\r\n",
      "  Max size of kernel argument                     4352 (4.25KiB)\r\n",
      "  Queue properties                                \r\n",
      "    Out-of-order execution                        Yes\r\n",
      "    Profiling                                     Yes\r\n",
      "  Prefer user sync for interop                    No\r\n",
      "  Profiling timer resolution                      1000ns\r\n",
      "  Execution capabilities                          \r\n",
      "    Run OpenCL kernels                            Yes\r\n",
      "    Run native kernels                            No\r\n",
      "    Kernel execution timeout (NV)                 Yes\r\n",
      "  Concurrent copy and kernel execution (NV)       Yes\r\n",
      "    Number of async copy engines                  2\r\n",
      "  printf() buffer size                            1048576 (1024KiB)\r\n",
      "  Built-in kernels                                \r\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_nv_create_buffer\r\n",
      "\r\n",
      "NULL platform behavior\r\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\r\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\r\n",
      "  clCreateContext(NULL, ...) [default]            No platform\r\n",
      "  clCreateContext(NULL, ...) [other]              Success [INTEL]\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No platform\r\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\r\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Workitems vs Workgroups\n",
    "Exploring different types of parallelism, with our trusty vector sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## OpenCL Task vs Data Parallelism Abstractions\n",
    "\n",
    "![OpenCL Task vs Data Abstractions](./img/opencl_task_data_parallelism.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectorising\n",
    "\n",
    "```c\n",
    "kernel void sum16(global float16 *a, \n",
    "                  global float16 *b,\n",
    "                  global float16 *c){\n",
    "  int gid = get_global_id(0);\n",
    "    \n",
    "  c[gid] = a[gid] + b[gid];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.36 ms ± 528 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectorised + Local Memory\n",
    "\n",
    "```c\n",
    "kernel void sum16_local(global float16 *a, \n",
    "                        global float16 *b,\n",
    "                        global float16 *c){\n",
    "  int wid = get_group_id(0)*WG_SIZE;\n",
    "  int lid = get_local_id(0);\n",
    "    \n",
    "  local float16 a_local[WG_SIZE], b_local[WG_SIZE], c_local[WG_SIZE];\n",
    "    \n",
    "  // Copying on\n",
    "  event_t copyon[2];\n",
    "  copyon[0] = async_work_group_copy(a_local, a + wid, WG_SIZE, 0);\n",
    "  copyon[1] = async_work_group_copy(b_local, b + wid, WG_SIZE, 0);\n",
    "  wait_group_events(2, copyon);\n",
    "    \n",
    "  c_local[lid] = a_local[lid] + b_local[lid];\n",
    "    \n",
    "  // Copying off\n",
    "  event_t copyoff[1];\n",
    "  copyoff[0] = async_work_group_copy(c + wid, c_local, WG_SIZE, 0);\n",
    "  wait_group_events(1, copyoff);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.97 ms ± 51.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16_local, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But, before we think that we're too clever:\n",
    "\n",
    "What about good old `numpy`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668 µs ± 157 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Increasing the computational work\n",
    "\n",
    "Now we make the kernel: $\\vec{c} = {(\\vec{a} + \\vec{b}) }^{x}$\n",
    "\n",
    "```c\n",
    "kernel void sum16_pow(global float16 *a, \n",
    "                      global float16 *b,\n",
    "                      global float16 *c){\n",
    "  int gid = get_global_id(0);\n",
    "    \n",
    "  c[gid] = pow(a[gid] + b[gid], POW);\n",
    "}\n",
    "```\n",
    "\n",
    "Make $x=1000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "power = 1000\n",
    "build_options = [\"-D\",\"BATCH_SIZE={}\".format(batch_size),\n",
    "                 \"-D\",\"WG_SIZE={}\".format(wg_size),\n",
    "                 \"-D\",\"POW={}\".format(power)]\n",
    "\n",
    "nvidia_program = build_program(\"vector_sum.cl\", nvidia_context, build_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26 ms ± 538 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_ocl_kernel(nvidia_queue, nvidia_program.sum16_pow, (N//16,), input_tuples, output_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gordon/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in power\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (a+b)**power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ca(t)veats\n",
    "\n",
    "![Cat no gif](./img/cat_no.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Amdahl's law](./img/amdahl_graph.png)\n",
    "\n",
    "Source <a href=\"https://en.wikipedia.org/wiki/User:Daniels220\" class=\"extiw\" title=\"wikipedia:User:Daniels220\">Daniels220</a> at <a href=\"https://en.wikipedia.org/wiki/\" class=\"extiw\" title=\"wikipedia:\">English Wikipedia</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=6678551\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![XKCD is it worth the time?](./img/xkcd_is_it_worth_the_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you!\n",
    "\n",
    "![Cat thank you](./img/cat_thankyou.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommended Reading List\n",
    "* Gene Amdahl, Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities\n",
    "* Page and Luk, Compiling Occam into field-programmable gate arrays\n",
    "* Herb Sutter, The Free Lunch is Over\n",
    "* Asanovic et al., The Landscape of Parallel Computing Research: A View from Berkeley\n",
    "* Tsugio Makimoto, The Hot Decade of Field Programmable Technologies\n",
    "* Lee et al., Debunking the 100X GPU vs CPU myth\n",
    "* Che et al., Rodinia: A Benchmark Suite for Heterogeneous Computing\n",
    "* Thomas et al., Hardware architectures for Monte-Carlo based financial simulations\n",
    "* Mike Giles, Some (strong) opinions on HPC and the use of GPUs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
